#!/usr/bin/env pandda.python


###############################################################################
# Packages
##############################################################################
import os, sys, copy, glob
import shutil
import pandas
import libtbx.phil
import scipy.stats as stats
import numpy
#################################
import matplotlib
matplotlib.use('Agg')
matplotlib.interactive(0)
from matplotlib import pyplot
pyplot.style.use('ggplot')
##################################

from scipy.cluster.hierarchy import dendrogram
from scipy.stats import entropy

# Package for Non linear least squares curve fitting 
from scipy.optimize import curve_fit
         
from bamboo.common.command import CommandManager

###############################################################################
# Set up for passing arguments
############################################################################### 

blank_arg_prepend = {None:'dir=', '.pdb':'pdb=', '.mtz':'mtz='}

master_phil = libtbx.phil.parse("""
input {
    dir = None
        .type = path
        .multiple = True
    pdb_style = "*.dimple.pdb"
        .type = str
        .multiple = False
    mtz_style = "*.dimple.mtz"
      .type = str
        .multiple = False
}
output {
    log = "ringer.log"
        .type = str
        .multiple = False
}
settings {
    # XXX mmtbx.ringer can only take this an integer, >1 XXX#
    angle_sampling = 2
        .type = int
        .multiple = False
}
""")

############################################################################################
#                                   FUNCTIONS                                              #
############################################################################################

def process_with_ringer(pdb, mtz, angle_sampling, output_dir=None, output_base=None):
    """Analyse a pdb-mtz pair with mmtbx.ringer"""    
    
    assert os.path.exists(pdb), 'PDB File does not exist'
    assert os.path.exists(mtz), 'MTZ File does not exist'

    if not output_dir:  output_dir = os.path.dirname(pdb)
    if not output_base: output_base = os.path.splitext(os.path.basename(pdb))[0]

    # Check/create output directory
    if not os.path.exists(output_dir): os.mkdir(output_dir)

    output_csv = os.path.join(output_dir, output_base+'.csv')
    # Import pandas for utilising dataframes
    import pandas
    ###################################################
    #Run Ringer
    ###################################################

    # Only run if results don't already exist
    if not os.path.exists(output_csv):
        # Initialise and populate command object
        ringer = CommandManager(program='/usr/local/phenix/phenix-1.9-1682/build/intel-linux-2.6-x86_64/bin/mmtbx.ringer')
        ringer.add_command_line_arguments(pdb, mtz)
        ringer.add_command_line_arguments('sampling_angle={}'.format(angle_sampling))
        ringer.add_command_line_arguments('output_base={}'.format(os.path.join(output_dir, output_base)))
        # Print and run
        ringer.print_settings()
        ringer.run()
        # Write log
        ringer.write_output(os.path.join(output_dir, output_base+'.log'))

    # Check the output csv file exists
    output_csv = os.path.join(output_dir, output_base+'.csv')
    assert os.path.exists(output_csv), 'Ringer output CSV does not exist: {}'.format(output_csv)

    return output_csv

def normalise_and_sort_ringer_results(current_dataset_results,params):
    #Extract from current residue in dataset
    residue = current_dataset_results.index[0]
    start_ang  = current_dataset_results.values[0,2]
    ang_rel = params.settings.angle_sampling*current_dataset_results.columns.values[3:]-3
    map_values = current_dataset_results.values[0,3:]

    print 'Showing data for {}'.format(residue)
    # Set angles
    ang = (start_ang+ang_rel)%360
   
    ###################################################
    # Sort Angles
    ##################################################
    sorted_idx = sorted(range(len(ang)), key=lambda i: ang[i])
    sorted_angles = [ang[i] for i in sorted_idx]
    sorted_map_values = [map_values[i] for i in sorted_idx]
    
    return (sorted_angles,sorted_map_values)

def line_plot_ringer(sorted_angles,sorted_map_values,title,filename,out_dir):

    fig=pyplot.figure()
    pyplot.plot(sorted_angles, sorted_map_values)
    pyplot.title(title)
    pyplot.xlabel('Angle')
    pyplot.tight_layout()
    pyplot.savefig(os.path.join(out_dir,filename))
    pyplot.close(fig)

def multiple_line_plot_ringer(all_data_list,title, filename, out_dir):

    fig=pyplot.figure()
    pyplot.title(title)
    
    for i in range(0,len(all_data_list)):
        sorted_angles=all_data_list[i][0]
        sorted_map_values=all_data_list[i][1]
        pyplot.plot(sorted_angles,sorted_map_values)
    
    pyplot.xlabel('Angle')
    pyplot.tight_layout()
    pyplot.savefig(os.path.join(out_dir,filename))
    pyplot.close(fig)

def linear_interpolate_ringer_results(sorted_angles,sorted_map_values,angle_sampling):
    # Import numpy for interpolation routines
    import numpy

    # Extend the map values, and angles to include the first element at end 
    # (over 360 deg),and the last elment at the start (below 0 deg)
    sorted_map_values.insert(0,sorted_map_values[-1])
    # now need to append 2nd value to end of list, as 1st values is the appended value
    sorted_map_values.append(sorted_map_values[1])
    sorted_angles.insert(0,sorted_angles[0]-angle_sampling)
    sorted_angles.append(sorted_angles[-1]+angle_sampling)  

    # Generate a set of angles to interpolate to based on the angle sampling
    interpolated_angles = numpy.arange(1, 360, angle_sampling)
    
    # interpolate
    interpolated_map_values = numpy.interp(interpolated_angles,sorted_angles,sorted_map_values) 

    return (interpolated_angles,interpolated_map_values)


###############################################################################
# Rendunant functions for correlation coefficents & clustering
###############################################################################
def correlation_single_residue(input_csv,residue,output_dir):
    # Import numpy for correlation routines, pandas for dataframes
    import numpy, pandas
    
    data = pandas.read_csv(input_csv)
    # Allocate array to store map values across datasets. Removing one from 
    # number of columns due to dataset name stored with map values
    all_map_values=numpy.zeros((len(data.values),len(data.columns)-1))
    dataset_labels=[]
    
    # Extract map values and dataset labels from Pandas import of CSV 
    for i in range(0,len(data.values)):
        dataset_labels.append(data.values[i,0])
        map_values = data.values[i,1:]  
        all_map_values[i]=map_values
    # Generate correlation coefficents
    correlation_matrix = numpy.corrcoef(all_map_values)
    # Store in labelled data frame
    correlation_data = pandas.DataFrame(correlation_matrix, index = dataset_labels, columns = dataset_labels)
    # Correlation data as CSV 
    filename='{}-from {} datasets-correlation-ringer.csv'.format(residue,len(data.values)) 
    correlation_csv = correlation_data.to_csv(os.path.join(output_dir,filename))

    return correlation_csv

    #from IPython import embed; embed() 

def correlation_heatmap(correlation_csv,residue,out_dir,min_corr):
    import numpy, pandas
    # Load csv into pandas DataFrame
    data = pandas.read_csv(correlation_csv, index_col=0)
    dataset_labels = data.columns.values
    
    # Make figure & plot: scale minimum to minimum across all datasets
    fig, ax = pyplot.subplots()
    heatmap= ax.pcolor(data.values,cmap='RdBu',vmin=round(min_corr-0.05,1),vmax=1)
   
    pyplot.title('{}'.format(residue),fontsize=300)
 
    # Format
    fig = pyplot.gcf()
    fig.set_size_inches(200,200)
    
    #Turn off the frame
    ax.set_frame_on(False)

    # Set tick positions
    ax.set(xticks=numpy.arange(len(dataset_labels))+0.5,xticklabels=dataset_labels,yticks = numpy.arange(len(dataset_labels))+0.5,yticklabels=dataset_labels)      #ax.set_xticks()

    #Set a table like display
    ax.invert_yaxis()
 
    # Set labels
    column_labels=dataset_labels
    row_labels=dataset_labels
    
    # Rotate x ticks
    pyplot.xticks(rotation=90)
    
    # Colourbar
    cbar = fig.colorbar(heatmap,shrink=0.5) #,label='Correlation Coefficient')
    cbar.ax.tick_params(labelsize=100)
    cbar.set_label('Correlation Coefficient',rotation=270,fontsize=100)
    #Output file
    filename = "{} correlation_plot.png".format(residue)
    pyplot.savefig(os.path.join(out_dir,filename))     
    pyplot.close(fig)    

def find_correlation_min(ref_set,out_dir,params):
    import numpy, pandas
    corrmin=[]

    for residue, data in ref_set.iterrows():
        out_dir_residue=os.path.join(out_dir,residue)
        correlation_csv='{}-from {} datasets-correlation-ringer.csv'.format(residue,len(params.input.dir))
        input_csv=os.path.join(out_dir_residue,correlation_csv) 
        # Load csv into pandas DataFrame
        data = pandas.read_csv(input_csv, index_col=0)
        dataset_labels = data.columns.values
        
        corrmin.append(min(data.min()))
    return min(corrmin)   

def augmented_dendrogram(*args, **kwargs):
    """Improved dendrogram plotting for Hierarchal clustering"""
    ddata = dendrogram(*args, **kwargs)

    if not kwargs.get('no_plot', False):
        for i, d in zip(ddata['icoord'], ddata['dcoord']):
            x = 0.5 * sum(i[1:3])
            y = d[1]
            pyplot.plot(x, y, 'ro')
            pyplot.annotate("%.3g" % y, (x, y), xytext=(0, -8),
                         textcoords='offset points',
                         va='top', ha='center')
    return ddata

def cluster_single_residue(correlation_csv,residue,out_dir):
    """ Hierarchal clustering on correlation matrix """
    import numpy, scipy, pandas, re
    
    from scipy.cluster.hierarchy import linkage
    from scipy.cluster.hierarchy import fcluster    

    # Load csv into pandas DataFrame
    data = pandas.read_csv(correlation_csv, index_col=0)
    dataset_labels = data.columns.values

    # Generate linkage matrix 
    linkage_matrix = linkage(data.values, "single")
    
    # Cluster selection
    #fcluster(linkage_matrix, 1)
 
    # Plotting dendorgram
    pyplot.figure(2, figsize=(8,8))
    pyplot.clf()
    ddata = augmented_dendrogram(linkage_matrix,color_threshold=1,p=15,truncate_mode='lastp',labels=dataset_labels)
    "{}_correlation_dendrogram".format(residue)
    pyplot.gcf().subplots_adjust(bottom=0.25)
    pyplot.xticks(rotation=90)
    pyplot.title("{} Correlation Dendrogram".format(residue))
    pyplot.ylabel("Cophenetic distance")
    plot_filename = "{}_correlation_dendrogram.png".format(residue)
    pyplot.savefig(os.path.join(out_dir,plot_filename))
        
 
    # Getting labels for singletons from dendrogram
    cluster_leaves = ddata.get('ivl')
    regex=re.compile('\(.')
    #singletons = filter(lambda i: not regex.search(i), cluster_leaves) 
    singletons = [i for i in cluster_leaves if not regex.search(i)]
    #Get maximal cophenetic distance
    maximal_cophenetic = ddata.get('dcoord')[-1][1]
    
    return maximal_cophenetic    


def correlation_coefficent_clustering(ref_set,out_dir,map_type,angle_type,params):

    # TODO Send to file, so this doesn't run every rub
    # Find miminum value in correlation_plots   
    min_corr = find_correlation_min(ref_set,out_dir,params)
    
    # DataFrame to store max_cophenetic distance
    max_coph=[]

    # Calculate correlation coefficents and cluster for single residues    
    for residue, data in ref_set.iterrows():
        out_dir_residue=os.path.join(out_dir,residue)

        interpolate_csv = '{}_{}_Datasets_{}_{}-ringer.csv'.format(residue,len(params.input.dir),map_type,angle_type)
        input_csv = os.path.join(out_dir_residue,interpolate_csv)
        
        correlation_csv='{}-from {} datasets-correlation-ringer.csv'.format(residue,len(params.input.dir))

        if not os.path.exists(os.path.join(out_dir_residue,correlation_csv)):
            correlation_data = correlation_single_residue(input_csv,residue,out_dir_residue)
        else:
            print'{}: Correlation CSV already generated, for these {} datasets'.format(residue,len(params.input.dir))
        # print correlation heatmap 
        correlation_input_csv = os.path.join(out_dir_residue,correlation_csv) 
        if not os.path.exists(os.path.join(out_dir_residue,"{} correlation_plot.png".format(residue))):    
            correlation_heatmap(correlation_input_csv,residue,out_dir_residue,min_corr)
        else:
            print '{}: Correlation Heatmap already generated, for these {} datasets'.format(residue,len(params.input.dir))       

        #Hierachical clustering across one residue
        plot_filename = "{}_correlation_dendrogram.png".format(residue)
        #if not os.path.exists(os.path.join(out_dir_residue,plot_filename)):
        max_cophenetic = cluster_single_residue(os.path.join(out_dir_residue,correlation_csv),residue,out_dir_residue)
        #else:
        #    print '{}: Correlation dendrogram already generated for these {} datasets'.format(residue,len(params.input.dir))
        #max_coph.append([residue,max_cophenetic])
    
   # max_coph_df=pandas.DataFrame(max_coph)    
   # max_coph_df.to_csv('test.csv')
    
###############################################################################
# Main program 
###############################################################################

def run(params):

    # Dictionary to store all of the 
    # ringer results for each of the 
    # datasets
    all_results = {}

    for dir in params.input.dir:
        # Label the dataset by the directory name
        label = os.path.basename(dir)
        pdb = glob.glob(os.path.join(dir, params.input.pdb_style))
        mtz = glob.glob(os.path.join(dir, params.input.mtz_style))
        assert pdb, 'No PDB Files found in {} matching {}'.format(dir, params.input.pdb_style)
        assert mtz, 'No MTZ Files found in {} matching {}'.format(dir, params.input.mtz_style)
        pdb = pdb[0]
        mtz = mtz[0]

        # Process dataset with ringer and convert results to DataFrame
        ringer_csv = process_with_ringer(pdb=pdb, mtz=mtz,angle_sampling=params.settings.angle_sampling, output_dir=dir)
        ringer_results = pandas.DataFrame.from_csv(ringer_csv, header=None)
        all_results[label] = ringer_results
        
    # Pull out the "first" ringer results set as a reference
    ref_set = all_results.values()[0]

    # Map and angle types currently selected to analyse
    map_type = '2mFo-DFc'
    angle_type = 'chi1'

    # Create an output directory if it doesn't already exist 
    out_dir = 'Processed_data'
    if not os.path.isdir(out_dir):
        os.makedirs(out_dir)

    #Choose a map_type/angle_type by reducing reference set
    ref_set=ref_set.loc[(ref_set[1] == map_type)]
    ref_set=ref_set.loc[(ref_set[2] == angle_type)]

    # Iterate through the residues
    for residue, data in ref_set.iterrows():
       residue_data_list = []
        
        # Create ouput directories for each residue
       out_dir_residue=os.path.join(out_dir,residue)
       if not os.path.isdir(out_dir_residue):
           os.makedirs(out_dir_residue)
        # Output filename for interpolated data 
        # (Used to check existence of output)
       interpolate_csv = '{}_{}_Datasets_{}_{}-ringer.csv'.format(residue,len(params.input.dir),map_type,angle_type)
        
       if not os.path.exists(os.path.join(out_dir_residue,interpolate_csv)):
        
            # Iterate through the datasets
           for dataset_label, dataset_results in all_results.iteritems():
                 
               current_dataset_results = dataset_results.loc[(dataset_results.index == residue)]    

               sorted_angles, sorted_map_values = normalise_and_sort_ringer_results(current_dataset_results, params=params)
               line_plot_ringer(sorted_angles, sorted_map_values, title=residue, filename='{}-{}.png'.format(residue, dataset_label),out_dir=out_dir_residue)

               interpolated_angles,interpolated_map_values = linear_interpolate_ringer_results(sorted_angles,sorted_map_values,angle_sampling=params.settings.angle_sampling)          
                
               # Store these in a list
               residue_data_list.append((interpolated_angles, interpolated_map_values))
                
               # If it doesn't exist: Create dataframe to store results from one residue, across multiple datasets 
               if not 'single_residue_multiple_datasets' in locals():
                   single_residue_multiple_datasets = pandas.DataFrame(columns=interpolated_angles)

               # Populate dataframe with results from one residue, across multiple datasets 
               single_residue_multiple_datasets.loc['{}'.format(dataset_label)]=interpolated_map_values        

            # Print results for all of the datasets for this residue in the same graph
            # Output CSV from one resiude, multiple datasets
           multiple_line_plot_ringer(residue_data_list,title=residue, filename='all-{}-{}-dataset.png'.format(residue, len(residue_data_list)),out_dir=out_dir_residue)  
           pandas.DataFrame.to_csv(single_residue_multiple_datasets,os.path.join(out_dir_residue,interpolate_csv))
       else:
           print '{}:Interpolated CSVs already generated, for these {} datasets'.format(residue,len(params.input.dir))

    
    # Reduntant function that generates clustering coefficent clustering 
    #correlation_coefficent_clustering(ref_set,out_dir,map_type,angle_type,params)
    
    #######################################################################
    # Generating histogram to show the location of the maximal points 
    # of peak in ringer plot. Shows three rotamer bins [0, 120, 240] 
    #######################################################################
    
    #TODO Check location of rotamer peaks (add to histogram)

    if not os.path.exists(os.path.join(out_dir,'Modal_peak_location.png')):

        modal_peak_angle=[]

        for residue, data in ref_set.iterrows():
            interpolate_csv = '{}_{}_Datasets_{}_{}-ringer.csv'.format(residue,len(params.input.dir),map_type,angle_type)
            out_dir_residue=os.path.join(out_dir,residue)        
            interpolated_results=pandas.read_csv(os.path.join(out_dir_residue,interpolate_csv), index_col=0)

            angle_with_max_map=(interpolated_results.idxmax(axis=1).values).astype(numpy.float)
            modal_peak_angle.append(angle_with_max_map) 
        pyplot.hist(numpy.concatenate(modal_peak_angle,axis=0),bins=24, normed=True)
        pyplot.xlabel('Angle')
        pyplot.ylabel('Relative frequency')
        pyplot.title('Angles with maximal map value')
        pyplot.savefig(os.path.join(out_dir,"Modal_peak_location"))
        pyplot.close()
    else:
        print 'Histogram of peak angles exists'
    
    ###########################################################################
    # Non Linear Least squares fitting routine, single & multiple gaussian. 
    #  
    ###########################################################################

    for residue, data in ref_set.iterrows():
        
        # Reading in interpolated angle data
        interpolate_csv = '{}_{}_Datasets_{}_{}-ringer.csv'.format(residue,len(params.input.dir),map_type,angle_type)
        out_dir_residue=os.path.join(out_dir,residue)
        interpolated_results=pandas.read_csv(os.path.join(out_dir_residue,interpolate_csv), index_col=0)
            
        interpolated_row = interpolated_results.values[0]            
        interpolated_angles = interpolated_results.columns.values.astype(int)            

        # Single gaussain to fit data to (3 parameters)
        def single_1d_gaussian_func(x,a,x0,sigma):
            return a*numpy.exp(-(x-x0)**2/(2*sigma**2))
        # Three gaussian curve to fit data to (9 parameters)
        def three_1d_gaussian_func(x,a1,x01,sigma_1,a2,x02,sigma_2,a3,x03,sigma_3):
            return (a1*numpy.exp(-(x-x01)**2/(2*sigma_1**2))+
                       a2*numpy.exp(-(x-x02)**2/(2*sigma_2**2))+
                       a3*numpy.exp(-(x-x03)**2/(2*sigma_3**2)))


        # Initial parameter values for model fit
        initialise_three=[10,0,20,10,120,20,10,240,20]

        # Bounds (Added if updating scipy version)
        #bounds_three=(-(numpy.inf),numpy.inf)

        # Non linear least sqaures Curve fit on data:
        # popt returnd best fit values for parameters of the model
        # pcov contains covariance matrix for the fit
        popt, pcov = curve_fit(three_1d_gaussian_func, interpolated_angles,
                                interpolated_row,p0 = initialise_three, maxfev=1000000000)
        # Fitted Data
        y_fit = three_1d_gaussian_func(interpolated_angles, popt[0], 
                    popt[1], popt[2], popt[3], popt[4], popt[5], 
                    popt[6], popt[7], popt[8])
        
        # Figure (of curve fit)

        fig = pyplot.figure()
        ax = fig.add_subplot(111)
        ax.scatter(interpolated_angles,interpolated_row)
        ax.plot(interpolated_angles, y_fit, label= '3 gaussian fit')
        ax.legend()
        
        # Output image filename        
        filename='{}_multi_gaussian_fit.png'.format(residue)
            
        # output directory
        out_dir_fit = os.path.join(out_dir,'Three_Gaussian_fit')
        if not os.path.isdir(out_dir_fit):
            os.makedirs(out_dir_fit)

        fig.savefig(os.path.join(out_dir_fit,filename))
        pyplot.close()
        
        break    
        #from IPython import embed; embed() 


#For allowing command manager 
if __name__ == '__main__':
    from giant.jiffies import run_default
    run_default(run=run, master_phil=master_phil, args=sys.argv[1:], blank_arg_prepend=blank_arg_prepend)
